{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Compare LSTM and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Yiheng Xiao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use:\n",
    "- Long short term memory RNN (LSTM)\n",
    "- Gated Recurrent Units RNN (GRU)\n",
    "\n",
    "This will allow us to achieve ~86% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPU(s)!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPU(s)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "The same as before, we'll set the seed, define the `Fields` and get the train/valid/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(tensor_type=torch.FloatTensor)\n",
    "\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train, valid = train.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first update, is the addition of pre-trained word embeddings. These vectors have been trained on corpuses of billions of tokens. Now, instead of having our word embeddings initialized randomly, they are initialized with these pre-trained vectors, where words that appear in similar contexts appear nearby in this vector space.\n",
    "\n",
    "The first step to using these is to specify the vectors and download them, which is passed as an argument to `build_vocab`. The `glove` is the algorithm used to calculate the vectors, go [here](https://nlp.stanford.edu/projects/glove/) for more. `6B` indicates these vectors were trained on 6 billion tokens. `100d` indicates these vectors are 100-dimensional.\n",
    "\n",
    "**Note**: recently, 'glove.6B.100d' from https://nlp.stanford.edu/projects/glove/ crashed down. I use \"charngram.100d\" instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, max_size=25000, vectors = \"charngram.100d\")\n",
    "## ['charngram.100d', 'fasttext.en.300d', 'fasttext.simple.300d', 'glove.42B.300d', 'glove.840B.300d', 'glove.twitter.27B.25d', 'glove.twitter.27B.50d', 'glove.twitter.27B.100d', 'glove.twitter.27B.200d', 'glove.6B.50d', 'glove.6B.100d', 'glove.6B.200d', 'glove.6B.300d']\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we create the iterators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Please select suitable `BATCH_SIZE` according to the computation power of GPU. With larger internal memory, say 8G, you can raise the `BATCH_SIZE` to 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, valid, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.text), \n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LSTM, the hidden state can be thought of as a \"memory\" of the words seen by the model. It is difficult to train a standard RNN as the gradient decays exponentially along the sequence, causing the RNN to \"forget\" what has happened earlier in the sequence. LSTMs have an extra recurrent state called a _cell_, which can be thought of as the \"memory\" of the LSTM and can remember information for many time steps. LSTMs also use multiple _gates_, these control the flow of information into and out of the memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/knsIzeh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        param:\n",
    "            vocab_size: dimension of one-hot vector, which is length of TEXT.vocab\n",
    "            embedding_dim: dim(dense word vector) approx sqrt(cocab_size), after embedding, one-hot key vectors are converted to dense word vector , \n",
    "            hidden_dim: size of hidden states, subsequent words are fed to hidden states successively\n",
    "            output_dim: The output dimension is usually the number of classes, \n",
    "                however in the case of only 2 classes the output value is between 0 and 1 and thus can be 1-dimensional, \n",
    "                i.e. a single scalar.\n",
    "            n_layers: number of layers in the neural network.\n",
    "                The idea is that we add additional RNNs on top of the initial standard RNN, where each RNN added is another layer.\n",
    "            bidirectional: adds an extra layer that processes values from last to first, where originally only from first to last\n",
    "            dropout: a regularization method to avoid overfitting, randomly dropout a node from the forward process, getting less\n",
    "                parameters and hence avoid over parameterization\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM package that takes in specifications\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        # defines forward process, bidirectional requires the square of hidden dimension\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        return: returns the output and a tuple of the final hidden state and the final cell state, \n",
    "            whereas the standard RNN only returned the output and final hidden state.\n",
    "        \"\"\"\n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        # regularization in the embedding process\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        # output of the LSTM RNN process in each node, including the output, new hidden state, and cell state\n",
    "        output, (hidden, cell) = self.rnn(embedded)        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        # regularize the hidden state to avoid overfitting\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "\n",
    "class RNN_GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        same as LSTM, use GRU package instead\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        difference between LSTM and GRU here is that GRU output does not have cell state,\n",
    "        as we can see from mathematical definition above\n",
    "        \"\"\"\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model_lstm = RNN_LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "model_gru = RNN_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check size of pretrained embeddings\n",
    "\"\"\"\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.2320, -0.2417, -0.8178,  ...,  0.3751, -0.3104,  1.0578],\n",
       "        [-0.0731, -0.1633, -0.1367,  ..., -0.2390,  0.2612,  0.2366],\n",
       "        ...,\n",
       "        [-0.2472, -0.1776, -0.1791,  ...,  0.0936, -0.2388,  0.5512],\n",
       "        [-0.1875, -0.0243, -0.3291,  ..., -0.0950, -0.3361,  0.6293],\n",
       "        [-0.0925, -0.1457, -0.2654,  ..., -0.4071, -0.0766,  0.4163]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Assign pretrained embeddings to embedding layer for 2 separate models\n",
    "\"\"\"\n",
    "model_lstm.embedding.weight.data.copy_(pretrained_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\"\"\"\n",
    "change optim.SGD to optim.Adam, \n",
    "also note how we do not have to provide an initial learning rate for Adam as PyTorch specifies a sensibile initial learning rate.\n",
    "\"\"\"\n",
    "optimizer_lstm = optim.Adam(model_lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "specify loss function: BCE with logits loss\n",
    "\"\"\"\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\"\"\"\n",
    "use GPU if availbale, otherwise use CPU\n",
    "\"\"\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_lstm = model_lstm.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as F\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    main train function to train with each batch in iterator, iterates through all examples\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \"\"\"\n",
    "        optimization step\n",
    "        \"\"\"\n",
    "        \n",
    "        # first zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # feed batch of sentences to model\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        # calculate gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    main function for evaluation\n",
    "    similar to train\n",
    "    do not need to zero gradients\n",
    "    do not update parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM training set\n",
      "Epoch: 0001, Train Loss: 0.6973, Train Acc: 50.9255%, Val. Loss: 0.6909, Val. Acc: 51.9323%\n",
      "LSTM training set\n",
      "Epoch: 0002, Train Loss: 0.6744, Train Acc: 57.6325%, Val. Loss: 0.6317, Val. Acc: 65.5251%\n",
      "LSTM training set\n",
      "Epoch: 0003, Train Loss: 0.5072, Train Acc: 75.9426%, Val. Loss: 0.3264, Val. Acc: 87.2468%\n",
      "LSTM training set\n",
      "Epoch: 0004, Train Loss: 0.2683, Train Acc: 89.6881%, Val. Loss: 0.2729, Val. Acc: 89.1125%\n",
      "LSTM training set\n",
      "Epoch: 0005, Train Loss: 0.1772, Train Acc: 93.6586%, Val. Loss: 0.2732, Val. Acc: 89.7255%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\"\"\"\n",
    "train model for 5 epochs and output training statistics and validation statstics\n",
    "\"\"\"\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss_lstm, train_acc_lstm = train_model(model_lstm, train_iterator, optimizer_lstm, criterion)\n",
    "    valid_loss_lstm, valid_acc_lstm = evaluate(model_lstm, valid_iterator, criterion)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"LSTM training set\")\n",
    "    print(f'Epoch: {epoch+1:04}, Train Loss: {train_loss_lstm:.4f}, Train Acc: {train_acc_lstm*100:.4f}%, Val. Loss: {valid_loss_lstm:.4f}, Val. Acc: {valid_acc_lstm*100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM test result\n",
      "Test Loss: 0.340, Test Acc: 86.81%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "test final model\n",
    "\"\"\"\n",
    "test_loss_lstm, test_acc_lstm = evaluate(model_lstm, test_iterator, criterion)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"LSTM test result\")\n",
    "print(f'Test Loss: {test_loss_lstm:.3f}, Test Acc: {test_acc_lstm*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN_GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        same as LSTM specification, instead we use GRU package\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        difference between LSTM and GRU here is that GRU output does not have cell state,\n",
    "        as we can see from mathematical definition above\n",
    "        \"\"\"\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model_gru = RNN_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check size of pretrained embeddings\n",
    "\"\"\"\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-1.2320, -0.2417, -0.8178,  ...,  0.3751, -0.3104,  1.0578],\n",
       "        [-0.0731, -0.1633, -0.1367,  ..., -0.2390,  0.2612,  0.2366],\n",
       "        ...,\n",
       "        [-0.2472, -0.1776, -0.1791,  ...,  0.0936, -0.2388,  0.5512],\n",
       "        [-0.1875, -0.0243, -0.3291,  ..., -0.0950, -0.3361,  0.6293],\n",
       "        [-0.0925, -0.1457, -0.2654,  ..., -0.4071, -0.0766,  0.4163]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\"\"\"\n",
    "change optim.SGD to optim.Adam, \n",
    "also note how we do not have to provide an initial learning rate for Adam as PyTorch specifies a sensibile initial learning rate.\n",
    "\"\"\"\n",
    "optimizer_gru = optim.Adam(model_gru.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "specify loss function: BCE with logits loss\n",
    "\"\"\"\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\"\"\"\n",
    "use GPU if availbale, otherwise use CPU\n",
    "\"\"\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_gru = model_gru.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as F\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    main train function to train with each batch in iterator, iterates through all examples\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \"\"\"\n",
    "        optimization step\n",
    "        \"\"\"\n",
    "        \n",
    "        # first zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # feed batch of sentences to model\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        # calculate gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    main function for evaluation\n",
    "    similar to train\n",
    "    do not need to zero gradients\n",
    "    do not update parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU training set\n",
      "Epoch: 0001, Train Loss: 0.5897, Train Acc: 65.8307%, Val. Loss: 0.3630, Val. Acc: 84.1818%\n",
      "GRU training set\n",
      "Epoch: 0002, Train Loss: 0.2834, Train Acc: 88.5740%, Val. Loss: 0.2952, Val. Acc: 88.3662%\n",
      "GRU training set\n",
      "Epoch: 0003, Train Loss: 0.1771, Train Acc: 93.4129%, Val. Loss: 0.2381, Val. Acc: 90.0187%\n",
      "GRU training set\n",
      "Epoch: 0004, Train Loss: 0.1177, Train Acc: 95.7724%, Val. Loss: 0.2577, Val. Acc: 91.0581%\n",
      "GRU training set\n",
      "Epoch: 0005, Train Loss: 0.0860, Train Acc: 97.1549%, Val. Loss: 0.3565, Val. Acc: 88.8859%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss_gru, train_acc_gru = train_model(model_gru, train_iterator, optimizer_gru, criterion)\n",
    "    valid_loss_gru, valid_acc_gru = evaluate(model_gru, valid_iterator, criterion)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GRU training set\")\n",
    "    print(f'Epoch: {epoch+1:04}, Train Loss: {train_loss_gru:.4f}, Train Acc: {train_acc_gru*100:.4f}%, Val. Loss: {valid_loss_gru:.4f}, Val. Acc: {valid_acc_gru*100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN-GRU test result\n",
      "Test Loss: 0.3823, Test Acc: 87.6480%\n"
     ]
    }
   ],
   "source": [
    "test_loss_gru, test_acc_gru = evaluate(model_gru, test_iterator, criterion)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"RNN-GRU test result\")\n",
    "print(f'Test Loss: {test_loss_gru:.4f}, Test Acc: {test_acc_gru*100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_lstm(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction_lstm = F.sigmoid(model_lstm(tensor))\n",
    "    return prediction_lstm.item()\n",
    "\n",
    "def predict_sentiment_gru(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction_gru = F.sigmoid(model_gru(tensor))\n",
    "    return prediction_gru.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a bottom line from a review of Titanic, `Titanic, one of the greatest movies of all time, as it manages to capture audiences' heart also skilfully enrapturing us with the dedication shown by people.`. Lets judge the sentiment by LSTM and GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9940310120582581"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive\n",
    "predict_sentiment_lstm(\"Titanic, one of the greatest movies of all time, as it manages to capture audiences' heart also skilfully enrapturing us with the dedication shown by people.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict value  = 0.99, which is sufficiently close to 1, postive sentiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016002169577404857"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative\n",
    "predict_sentiment_lstm(\"An disappointing plot comes after a dull routine debut. The conclsion is boring.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict value  = 0.0016, which is sufficiently close to 0, negative sentiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991788268089294"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive\n",
    "predict_sentiment_gru(\"Titanic, one of the greatest movies of all time, as it manages to capture audiences' heart also skilfully enrapturing us with the dedication shown by people.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict value  = 0.9992, which is sufficiently close to 1, postive sentiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00265431497246027"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative\n",
    "predict_sentiment_gru(\"An disappointing plot comes after a dull routine debut. The conclsion is boring.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict value  = 0.0027, which is sufficiently close to 0, negative sentiment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
