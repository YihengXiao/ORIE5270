from scipy.optimize import minimize
import numpy as np



def Rosenbrock(x):
    """
    Rosenbrock function
    :param x: array(1xn)
    :return real value
    """
    n=len(x)
    if n<=1:
        return None
    res=0
    for i in range(n-1):
        res+=100*(x[i+1]-x[i]**2)**2+(1-x[i])**2
    return np.array(res)

def Rosen_gradient(x):
    """
    This is the calculate the gradident of Rosenbrock when n=3
    :param x: a list representing the input vector x=[x0,x1,x2]
    :return: a list representing the gradient vector
    """
    if len(x) != 3:
        return None
    else:
        return np.array([-400 * x[0] * x[1] + 400 * x[0] ** 3 - 2 * (1 - x[0]),
                     -400 * x[1] * x[2] + 400 * x[1] ** 3 - 2 * (1 - x[1]) + 200 * (x[1] - x[0] ** 2),
                     200 * (x[2] - x[1] ** 2)])


if __name__ == "__main__":
    """
    Search for global optimum of Rosenbrock function, starting for different initial points.
    """
    # set initial points x0
    x0 = [[1.3, 0.7, 0.8],[-1,-2,1],[-3,2,1],[2.2,3.3,-1.1],[100,100,100]]
    for i in range(len(x0)):
        print("initial guess:")
        print(x0[i])
        res = minimize(Rosenbrock, x0[i], method='BFGS', options={'xtol': 1e-10, 'disp': True},jac=Rosen_gradient)
        print(res.x)